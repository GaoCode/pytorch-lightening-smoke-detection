# Torch imports
import pytorch_lightning as pl
from pytorch_lightning.callbacks.early_stopping import EarlyStopping

# Other package imports
from argparse import ArgumentParser

# File imports
from model import LightningModel
from batched_tiled_dataloader import BatchedTiledDataModule, BatchedTiledDataloader



#####################
## Argument Parser
#####################

parser = ArgumentParser(description='Takes raw wildfire images and saves tiled images')

# Path args
parser.add_argument('--data-path', type=str, default='/userdata/kerasData/data/new_data/batched_tiled_data',
                    help='Path to batched & tiled data.')
parser.add_argument('--metadata-path', type=str, default='/userdata/kerasData/data/new_data/batched_tiled_data/metadata.pkl',
                    help='Path to metadata.pkl generated by generate_batched_tiled_data.py.')
parser.add_argument('--omit-images-path', type=str, default=None,
                    help='Path to omit_images.npy containing list of images that are incorrectly labeled.')
parser.add_argument('--train-split-path', type=str, default=None,
                    help='(Optional) Path to txt file with train image paths. Only works if train, val, and test paths are provided.')
parser.add_argument('--val-split-path', type=str, default=None,
                    help='(Optional) Path to txt file with val image paths. Only works if train, val, and test paths are provided.')
parser.add_argument('--test-split-path', type=str, default=None,
                    help='(Optional) Path to txt file with test image paths. Only works if train, val, and test paths are provided.')

# Dataloader args
parser.add_argument('--batch-size', type=int, default=1,
                    help='Batch size for training.')
parser.add_argument('--num-workers', type=int, default=0,
                    help='Number of workers for dataloader.')
parser.add_argument('--series-length', type=int, default=1,
                    help='Number of sequential video frames to process during training.')
parser.add_argument('--time-range-min', type=int, default=-2400,
                    help='Start time of fire images to consider during training. ')
parser.add_argument('--time-range-max', type=int, default=2400,
                    help='End time of fire images to consider during training (inclusive).')

# Model args
parser.add_argument('--learning-rate', type=float, default=0.001,
                    help='Learning rate for training.')
parser.add_argument('--no-lr-schedule', action='store_true',
                    help='Disables ReduceLROnPlateau learning rate scheduler. See PyTorch Lightning docs for more details.')
parser.add_argument('--no-freeze-backbone', action='store_true',
                    help='Disables freezing of layers on pre-trained backbone.')

# Training args
parser.add_argument('--min-epochs', type=int, default=10,
                    help='Min number of epochs to train for.')
parser.add_argument('--max-epochs', type=int, default=50,
                    help='Max number of epochs to train for.')
parser.add_argument('--no-auto-lr-find', action='store_true',
                    help='Disables auto learning rate finder. See PyTorch Lightning docs for more details.')
parser.add_argument('--no-early-stopping', action='store_true',
                    help='Disables early stopping based on validation loss. See PyTorch Lightning docs for more details.')
parser.add_argument('--no-sixteen-bit', action='store_true',
                    help='Disables use of 16-bit training to reduce memory. See PyTorch Lightning docs for more details.')
parser.add_argument('--no-stochastic-weight-avg', action='store_true',
                    help='Disables stochastic weight averaging. See PyTorch Lightning docs for more details.')
parser.add_argument('--gradient-clip-val', type=float, default=0.5,
                    help='Clips gradients to prevent vanishing or exploding gradients. See PyTorch Lightning docs for more details.')
parser.add_argument('--accumulate-grad-batches', type=int, default=16,
                    help='Accumulate multiple batches before calling loss.backward() to increase effective batch size. See PyTorch Lightning docs for more details.')

    
#####################
## Main
#####################
    
def main(# Path args
        data_path, 
        metadata_path, 
        omit_images_path=None, 
        train_split_path=None, 
        val_split_path=None, 
        test_split_path=None,

        # Dataloader args
        batch_size=1, 
        num_workers=0, 
        series_length=1, 
        time_range=(-2400,2400), 

        # Model args
        learning_rate=0.001,
        lr_schedule=True,
        freeze_backbone=True,

        # Trainer args 
        min_epochs=10,
        max_epochs=50,
        auto_lr_find=True,
        early_stopping=True,
        sixteen_bit=True,
        stochastic_weight_avg=True,
        gradient_clip_val=0,
        accumulate_grad_batches=1):
    
    # Initialize data_module
    data_module = BatchedTiledDataModule(
        # Path args
        data_path=data_path,
        metadata_path=metadata_path,
        train_split_path=train_split_path,
        val_split_path=val_split_path,
        test_split_path=test_split_path,
        
        # Dataloader args
        batch_size=batch_size,
        num_workers=num_workers,
        series_length=series_length,
        time_range=time_range)
    
    # Initialize model
    model = LightningModel(learning_rate=learning_rate,
                           lr_schedule=lr_schedule,
                           freeze_backbone=freeze_backbone)

    # Implement EarlyStopping
    early_stop_callback = EarlyStopping(
       monitor='val_loss',
       min_delta=0.00,
       patience=5,
       verbose=False,
       mode='max')

    # Initialize a trainer
    trainer = pl.Trainer(
        # Trainer args
        min_epochs=min_epochs,
        max_epochs=max_epochs,
        auto_lr_find=auto_lr_find,
        callbacks=[early_stop_callback] if early_stopping else None,
        precision=16 if sixteen_bit else 32,
        stochastic_weight_avg=stochastic_weight_avg,
        gradient_clip_val=gradient_clip_val,
        accumulate_grad_batches=accumulate_grad_batches,
        
        # Dev args
#         fast_dev_run=True, 
        overfit_batches=5,
        log_every_n_steps=1,
        checkpoint_callback=False,
        logger=False,
#         track_grad_norm=2,
#         weights_summary='full',
#         profiler="simple", # "advanced" "pytorch"
#         log_gpu_memory=True,
        gpus=1)
    
    # Auto find learning rate
    if auto_lr_find:
        trainer.tune(model)
        
    # Train the model
    trainer.fit(model, data_module)
    
    # Evaluate the best model on the test set
    trainer.test(model, data_module)

    
if __name__ == '__main__':
    args = parser.parse_args()
        
    main(# Path args
        data_path=args.data_path, 
        metadata_path=args.metadata_path, 
        omit_images_path=args.omit_images_path, 
        train_split_path=args.train_split_path, 
        val_split_path=args.val_split_path, 
        test_split_path=args.test_split_path,

        # Dataloader args
        batch_size=args.batch_size, 
        num_workers=args.num_workers, 
        series_length=args.series_length, 
        time_range=(args.time_range_min,args.time_range_max), 

        # Model args
        learning_rate=args.learning_rate,
        lr_schedule=not args.no_lr_schedule,
        freeze_backbone=not args.no_freeze_backbone,

        # Trainer args
        min_epochs=args.min_epochs,
        max_epochs=args.max_epochs,
        auto_lr_find=not args.no_auto_lr_find,
        early_stopping=not args.no_early_stopping,
        sixteen_bit=not args.no_sixteen_bit,
        stochastic_weight_avg=not args.no_stochastic_weight_avg,
        gradient_clip_val=args.gradient_clip_val,
        accumulate_grad_batches=args.accumulate_grad_batches)